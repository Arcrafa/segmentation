{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting / Importing trained MaskRCNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "from keras import backend as K\n",
    "\n",
    "ROOT_DIR = os.path.abspath('../')\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing model\n",
    "Now, we can load the model from the pb file and then use it to infere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mold_inputs(images,config):\n",
    "    \"\"\"Takes a list of images and modifies them to the format expected\n",
    "    as an input to the neural network.\n",
    "    images: List of image matrices [height,width,depth]. Images can have\n",
    "        different sizes.\n",
    "\n",
    "    Returns 3 Numpy matrices:\n",
    "    molded_images: [N, h, w, 3]. Images resized and normalized.\n",
    "    image_metas: [N, length of meta data]. Details about each image.\n",
    "    windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\n",
    "        original image (padding excluded).\n",
    "    \"\"\"\n",
    "    molded_images = []\n",
    "    image_metas = []\n",
    "    windows = []\n",
    "    for image in images:\n",
    "        # Resize image\n",
    "        # TODO: move resizing to mold_image()\n",
    "        molded_image, window, scale, padding, crop = utils.resize_image(\n",
    "            image,\n",
    "            min_dim=config.IMAGE_MIN_DIM,\n",
    "            min_scale=config.IMAGE_MIN_SCALE,\n",
    "            max_dim=config.IMAGE_MAX_DIM,\n",
    "            mode=config.IMAGE_RESIZE_MODE)\n",
    "        molded_image = mold_image(molded_image, config)\n",
    "        # Build image_meta\n",
    "        image_meta = compose_image_meta(\n",
    "            0, image.shape, molded_image.shape, window, scale,\n",
    "            np.zeros([config.NUM_CLASSES], dtype=np.int32))\n",
    "        # Append\n",
    "        molded_images.append(molded_image)\n",
    "        windows.append(window)\n",
    "        image_metas.append(image_meta)\n",
    "    # Pack into arrays\n",
    "    molded_images = np.stack(molded_images)\n",
    "    image_metas = np.stack(image_metas)\n",
    "    windows = np.stack(windows)\n",
    "    return molded_images, image_metas, windows\n",
    "\n",
    "def mold_image(images, config):\n",
    "    return images.astype(np.float32) - config.MEAN_PIXEL\n",
    "\n",
    "def compose_image_meta(image_id, original_image_shape, image_shape,\n",
    "                       window, scale, active_class_ids):\n",
    "    \"\"\"Takes attributes of an image and puts them in one 1D array.\n",
    "\n",
    "    image_id: An int ID of the image. Useful for debugging.\n",
    "    original_image_shape: [H, W, C] before resizing or padding.\n",
    "    image_shape: [H, W, C] after resizing and padding\n",
    "    window: (y1, x1, y2, x2) in pixels. The area of the image where the real\n",
    "            image is (excluding the padding)\n",
    "    scale: The scaling factor applied to the original image (float32)\n",
    "    active_class_ids: List of class_ids available in the dataset from which\n",
    "        the image came. Useful if training on images from multiple datasets\n",
    "        where not all classes are present in all datasets.\n",
    "    \"\"\"\n",
    "    meta = np.array(\n",
    "        [image_id] +                  # size=1\n",
    "        list(original_image_shape) +  # size=3\n",
    "        list(image_shape) +           # size=3\n",
    "        list(window) +                # size=4 (y1, x1, y2, x2) in image cooredinates\n",
    "        [scale] +                     # size=1\n",
    "        list(active_class_ids)        # size=num_classes\n",
    "    )\n",
    "    return meta\n",
    "\n",
    "def unmold_detections(detections, mrcnn_mask, image_shape, window):\n",
    "    \"\"\"Reformats the detections of one image from the format of the neural\n",
    "    network output to a format suitable for use in the rest of the\n",
    "    application.\n",
    "\n",
    "    detections: [N, (y1, x1, y2, x2, class_id, score)]\n",
    "    mrcnn_mask: [N, height, width, num_classes]\n",
    "    image_shape: [height, width, depth] Original size of the image before resizing\n",
    "    window: [y1, x1, y2, x2] Box in the image where the real image is excluding the padding.\n",
    "\n",
    "        Returns:\n",
    "        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\n",
    "        class_ids: [N] Integer class IDs for each bounding box\n",
    "        scores: [N] Float probability scores of the class_id\n",
    "        masks: [height, width, num_instances] Instance masks\n",
    "        \"\"\"\n",
    "    # How many detections do we have?\n",
    "    # Detections array is padded with zeros. Find the first class_id == 0.\n",
    "    zero_ix = np.where(detections[:, 4] == 0)[0]\n",
    "    N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\n",
    "    print('Number of detections: ',N)\n",
    "    print('Window: ',window)\n",
    "    # Extract boxes, class_ids, scores, and class-specific masks\n",
    "    boxes = detections[:N, :4]\n",
    "    print('boxes',boxes.shape,' ',boxes)\n",
    "    class_ids = detections[:N, 4].astype(np.int32)\n",
    "    print('Class_ids: ',class_ids.shape,' ',class_ids)\n",
    "    scores = detections[:N, 5]\n",
    "    print('Scores: ',scores.shape,' ',scores)\n",
    "    masks = mrcnn_mask[np.arange(N), :, :, class_ids]\n",
    "    print('Masks: ',masks.shape)# masks)\n",
    "    # Compute scale and shift to translate coordinates to image domain.\n",
    "    print(image_shape[0])\n",
    "    print(window[2] - window[0])\n",
    "    h_scale = image_shape[0] / (window[2] - window[0])\n",
    "    print('h_scale: ',h_scale)\n",
    "    w_scale = image_shape[1] / (window[3] - window[1])\n",
    "    print('w_scale: ',w_scale)\n",
    "    scale = min(h_scale, w_scale)\n",
    "    shift = window[:2]  # y, x\n",
    "    print('shift: ',shift)\n",
    "    scales = np.array([scale, scale, scale, scale])\n",
    "    print('scales: ',scales)\n",
    "    shifts = np.array([shift[0], shift[1], shift[0], shift[1]])\n",
    "    print('shifts: ',shifts)\n",
    "    # Translate bounding boxes to image domain\n",
    "    boxes = np.multiply(boxes - shifts, scales).astype(np.int32)\n",
    "    print('boxes: ',boxes.shape,' ',boxes)\n",
    "    # Filter out detections with zero area. Often only happens in early\n",
    "    # stages of training when the network weights are still a bit random.\n",
    "    exclude_ix = np.where(\n",
    "        (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\n",
    "    if exclude_ix.shape[0] > 0:\n",
    "        boxes = np.delete(boxes, exclude_ix, axis=0)\n",
    "        class_ids = np.delete(class_ids, exclude_ix, axis=0)\n",
    "        scores = np.delete(scores, exclude_ix, axis=0)\n",
    "        masks = np.delete(masks, exclude_ix, axis=0)\n",
    "        N = class_ids.shape[0]\n",
    "\n",
    "    # Resize masks to original image size and set boundary threshold.\n",
    "    full_masks = []\n",
    "    for i in range(N):\n",
    "        # Convert neural network mask to full size mask\n",
    "        full_mask = utils.unmold_mask(masks[i], boxes[i], image_shape)\n",
    "        full_masks.append(full_mask)\n",
    "    full_masks = np.stack(full_masks, axis=-1)\\\n",
    "        if full_masks else np.empty((0,) + masks.shape[1:3])\n",
    "\n",
    "    return boxes, class_ids, scores, full_masks\n",
    "\n",
    "def get_anchors(image_shape, config):\n",
    "    \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n",
    "    backbone_shapes = compute_backbone_shapes(config, image_shape)\n",
    "    # Cache anchors and reuse if image shape is the same\n",
    "    _anchor_cache = {}\n",
    "    if not tuple(image_shape) in _anchor_cache:\n",
    "        # Generate Anchors\n",
    "        a = utils.generate_pyramid_anchors(\n",
    "            config.RPN_ANCHOR_SCALES,\n",
    "            config.RPN_ANCHOR_RATIOS,\n",
    "            backbone_shapes,\n",
    "            config.BACKBONE_STRIDES,\n",
    "            config.RPN_ANCHOR_STRIDE)\n",
    "        # Keep a copy of the latest anchors in pixel coordinates because\n",
    "        # it's used in inspect_model notebooks.\n",
    "        # TODO: Remove this after the notebook are refactored to not use it\n",
    "        anchors = a\n",
    "        # Normalize coordinates\n",
    "        _anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2])\n",
    "    return _anchor_cache[tuple(image_shape)]\n",
    "\n",
    "def compute_backbone_shapes(config, image_shape):\n",
    "    \"\"\"Computes the width and height of each stage of the backbone network.\n",
    "\n",
    "    Returns:\n",
    "        [N, (height, width)]. Where N is the number of stages\n",
    "    \"\"\"\n",
    "    if callable(config.BACKBONE):\n",
    "        return config.COMPUTE_BACKBONE_SHAPE(image_shape)\n",
    "\n",
    "    # Currently supports ResNet only\n",
    "    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n",
    "    return np.array(\n",
    "        [[int(math.ceil(image_shape[0] / stride)),\n",
    "            int(math.ceil(image_shape[1] / stride))]\n",
    "            for stride in config.BACKBONE_STRIDES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nova_basic\n",
    "\n",
    "class inferNovaConfig(nova_basic.novaConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "config = inferNovaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/sample\n",
      "Loading eval data\n",
      "Loaded 20 images from ['eval'] data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12ee998d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = nova_basic.novaDataset()\n",
    "dataset.load_nova('../data/sample','eval')\n",
    "dataset.prepare()\n",
    "\n",
    "image=dataset.load_image(0)\n",
    "images = [image]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded.\n",
      "Processing 1 images\n",
      "image                    shape: (240, 255, 3)         min:    0.00000  max:  255.00000  uint8\n",
      "RGB image loaded and preprocessed.\n",
      "(1, 256, 256, 3)\n",
      "Images meta:  [[  0 240 255   3 256 256   3   8   0 248 255   1   0   0   0   0   0   0\n",
      "    0]]\n",
      "Tensor(\"input_image:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
      "Tensor(\"input_anchors:0\", shape=(?, ?, 4), dtype=float32)\n",
      "Tensor(\"input_image_meta:0\", shape=(?, 19), dtype=float32)\n",
      "Found  Tensor(\"output_detections:0\", shape=(1, 100, 6), dtype=float32)\n",
      "Found  Tensor(\"output_mrcnn_class:0\", shape=(?, 1000, 7), dtype=float32)\n",
      "Found  Tensor(\"output_mrcnn_bbox:0\", shape=(?, 1000, 7, 4), dtype=float32)\n",
      "Found  Tensor(\"output_mrcnn_mask:0\", shape=(?, 100, 28, 28, 7), dtype=float32)\n",
      "Found  Tensor(\"output_rois:0\", shape=(1, ?, ?), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndetections = sess.run(detectionsT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\\n#print(\\'Detections: \\',detections[0].shape, detections[0])\\nmrcnn_class = sess.run(mrcnn_classT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\\n#print(\\'Classes: \\',mrcnn_class[0].shape, mrcnn_class[0])\\nmrcnn_bbox = sess.run(mrcnn_bboxT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\\n#print(\\'BBoxes: \\',mrcnn_bbox[0].shape, mrcnn_bbox[0])\\nmrcnn_mask = sess.run(mrcnn_maskT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\\n#print(\\'Masks: \\',mrcnn_mask[0].shape )#, outputs1[0])\\nrois = sess.run(roisT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\\n#print(\\'Rois: \\',rois[0].shape, rois[0])\\n\\nresults = []\\nfor i, image in enumerate(images):\\n    print(\\'Calculating results for image#\\',i)\\n    final_rois, final_class_ids, final_scores, final_masks =    unmold_detections(detections[i], mrcnn_mask[i],\\n                                    image.shape, windows[i])\\n    results.append({\\n        \"rois\": final_rois,\\n        \"class_ids\": final_class_ids,\\n        \"scores\": final_scores,\\n        \"masks\": final_masks,\\n    })\\nr = results[0]\\n#print(r)\\nprint (r[\\'scores\\'][0])\\nprint (r[\\'class_ids\\'][0])\\nprint (r[\\'rois\\'][0])\\nprint (r[\\'masks\\'][0].shape)\\n\\nclass_names = [\"BG\",\"nuclei\"]\\nvisualize.display_instances(image, r[\\'rois\\'], r[\\'masks\\'], r[\\'class_ids\\'], class_names, r[\\'scores\\'], ax=get_ax())\\nprint(\\'Done\\')\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.gfile.FastGFile('../model.pb', 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        _ = tf.import_graph_def(graph_def, name='')\n",
    "print('Graph loaded.')\n",
    "\n",
    "print(\"Processing {} images\".format(len(images)))\n",
    "for im in images:\n",
    "    modellib.log(\"image\", im)\n",
    "print('RGB image loaded and preprocessed.')\n",
    "molded_images, image_metas, windows = mold_inputs(images, config)\n",
    "print(molded_images.shape)\n",
    "\n",
    "image_shape = molded_images[0].shape\n",
    "anchors = get_anchors(image_shape, config)\n",
    "image_anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n",
    "\n",
    "print('Images meta: ',image_metas)\n",
    "img_ph = sess.graph.get_tensor_by_name('input_image:0')\n",
    "print(img_ph)\n",
    "img_anchors_ph = sess.graph.get_tensor_by_name('input_anchors:0')\n",
    "print(img_anchors_ph)\n",
    "img_meta_ph = sess.graph.get_tensor_by_name('input_image_meta:0')\n",
    "print(img_meta_ph)\n",
    "detectionsT = sess.graph.get_tensor_by_name('output_detections:0')\n",
    "print('Found ',detectionsT)\n",
    "mrcnn_classT = sess.graph.get_tensor_by_name('output_mrcnn_class:0')\n",
    "print('Found ',mrcnn_classT)\n",
    "mrcnn_bboxT = sess.graph.get_tensor_by_name('output_mrcnn_bbox:0')\n",
    "print('Found ', mrcnn_bboxT)\n",
    "mrcnn_maskT = sess.graph.get_tensor_by_name('output_mrcnn_mask:0')\n",
    "print('Found ', mrcnn_maskT)\n",
    "roisT = sess.graph.get_tensor_by_name('output_rois:0')\n",
    "print('Found ', roisT)\n",
    "\n",
    "detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rois = \\\n",
    "    sess.run([detectionsT, mrcnn_classT, mrcnn_bboxT, mrcnn_maskT, roisT],\n",
    "        feed_dict={img_ph: molded_images, img_meta_ph: image_metas, img_anchors_ph:image_anchors})\n",
    "'''\n",
    "detections = sess.run(detectionsT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Detections: ',detections[0].shape, detections[0])\n",
    "mrcnn_class = sess.run(mrcnn_classT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Classes: ',mrcnn_class[0].shape, mrcnn_class[0])\n",
    "mrcnn_bbox = sess.run(mrcnn_bboxT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('BBoxes: ',mrcnn_bbox[0].shape, mrcnn_bbox[0])\n",
    "mrcnn_mask = sess.run(mrcnn_maskT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Masks: ',mrcnn_mask[0].shape )#, outputs1[0])\n",
    "rois = sess.run(roisT, feed_dict={img_ph: molded_images, img_meta_ph: image_metas})\n",
    "#print('Rois: ',rois[0].shape, rois[0])\n",
    "\n",
    "results = []\n",
    "for i, image in enumerate(images):\n",
    "    print('Calculating results for image#',i)\n",
    "    final_rois, final_class_ids, final_scores, final_masks =\\\n",
    "    unmold_detections(detections[i], mrcnn_mask[i],\n",
    "                                    image.shape, windows[i])\n",
    "    results.append({\n",
    "        \"rois\": final_rois,\n",
    "        \"class_ids\": final_class_ids,\n",
    "        \"scores\": final_scores,\n",
    "        \"masks\": final_masks,\n",
    "    })\n",
    "r = results[0]\n",
    "#print(r)\n",
    "print (r['scores'][0])\n",
    "print (r['class_ids'][0])\n",
    "print (r['rois'][0])\n",
    "print (r['masks'][0].shape)\n",
    "\n",
    "class_names = [\"BG\",\"nuclei\"]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'], ax=get_ax())\n",
    "print('Done')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 6)\n",
      "(1, 1000, 7)\n",
      "(1, 1000, 7, 4)\n",
      "(1, 100, 28, 28, 7)\n",
      "(1, 1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(detections.shape)\n",
    "print(mrcnn_class.shape)\n",
    "print(mrcnn_bbox.shape)\n",
    "print(mrcnn_mask.shape)\n",
    "print(rois.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.9232206e-03, 2.1767330e-03, 9.7380316e-01, 2.0304220e-03,\n",
       "       9.7126508e-04, 1.7662117e-02, 4.3310277e-04], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrcnn_class[0][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19620506, 0.01870974, 0.6036592 , 0.26337546, 2.        ,\n",
       "       0.97380316], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19545679, 0.02805722, 0.5970976 , 0.26454562], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.3239254 ,  0.06280455, -0.10303418,  0.33728734],\n",
       "       [ 0.1408026 , -0.28707498,  0.2026481 ,  0.23306167],\n",
       "       [ 0.02454042, -0.14768033,  0.10552421,  0.30594844],\n",
       "       [ 0.23926234, -0.06561389, -0.3906288 , -0.34427735],\n",
       "       [-0.46545222,  0.18579829, -0.4898569 , -0.6205792 ],\n",
       "       [ 0.12163846, -0.2983503 , -0.28613093, -0.02090784],\n",
       "       [ 0.2647211 , -0.12588654, -0.10868292,  0.24211803]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrcnn_bbox[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.41673911e-01, 2.09830559e-05, 7.89885326e-06, ...,\n",
       "         1.80612224e-05, 1.89063339e-05, 6.68492430e-05],\n",
       "        [1.38529420e-01, 6.51002574e-06, 1.05241850e-06, ...,\n",
       "         2.13068301e-06, 1.38524297e-06, 4.45478008e-06],\n",
       "        [2.15829849e-01, 3.95630515e-08, 1.49000670e-08, ...,\n",
       "         3.62769264e-08, 3.29295560e-08, 2.25161230e-07],\n",
       "        ...,\n",
       "        [3.24926972e-01, 5.53620122e-02, 5.71050718e-02, ...,\n",
       "         4.45431992e-02, 6.55668303e-02, 6.03360981e-02],\n",
       "        [5.23662210e-01, 2.49404952e-01, 2.23479629e-01, ...,\n",
       "         2.01233879e-01, 2.64688402e-01, 2.15894029e-01],\n",
       "        [3.71213555e-01, 1.32080317e-01, 1.51684433e-01, ...,\n",
       "         1.17888048e-01, 1.59972534e-01, 1.37254700e-01]],\n",
       "\n",
       "       [[6.13143325e-01, 4.43234558e-06, 9.97602228e-07, ...,\n",
       "         1.03272953e-06, 5.87859677e-06, 5.18195793e-06],\n",
       "        [9.12425637e-01, 3.99717010e-07, 7.01528364e-08, ...,\n",
       "         1.15577130e-07, 1.97388587e-07, 2.02472438e-06],\n",
       "        [5.79440594e-01, 1.60116720e-09, 1.98754041e-10, ...,\n",
       "         2.17128829e-10, 1.73489423e-09, 3.40769857e-09],\n",
       "        ...,\n",
       "        [7.76373327e-01, 1.59127474e-01, 1.56776354e-01, ...,\n",
       "         1.17246002e-01, 1.58030137e-01, 1.05869696e-01],\n",
       "        [6.36530936e-01, 1.99004248e-01, 1.96742222e-01, ...,\n",
       "         1.62379101e-01, 1.77043796e-01, 1.64812967e-01],\n",
       "        [5.74701011e-01, 6.33190721e-02, 7.34452680e-02, ...,\n",
       "         6.65882155e-02, 7.86368400e-02, 8.59061107e-02]],\n",
       "\n",
       "       [[2.63682872e-01, 4.97597590e-08, 1.37468108e-08, ...,\n",
       "         3.70772604e-08, 2.76562080e-08, 3.14968844e-07],\n",
       "        [6.04195595e-02, 3.79426801e-09, 2.05005291e-10, ...,\n",
       "         5.68488423e-10, 3.49275581e-10, 4.68510786e-09],\n",
       "        [1.80897534e-01, 2.86374245e-12, 3.02985799e-13, ...,\n",
       "         1.33493791e-12, 8.69849440e-13, 2.33926888e-11],\n",
       "        ...,\n",
       "        [4.28733230e-01, 1.60013646e-01, 2.35690936e-01, ...,\n",
       "         1.26759619e-01, 2.31522486e-01, 2.15640664e-01],\n",
       "        [4.59689438e-01, 4.26384807e-02, 5.05744927e-02, ...,\n",
       "         4.06371467e-02, 4.41883877e-02, 4.86433730e-02],\n",
       "        [2.96117395e-01, 3.84328491e-03, 3.77132487e-03, ...,\n",
       "         4.18129284e-03, 6.06828649e-03, 6.53244741e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[5.75216234e-01, 5.84360538e-03, 3.86674190e-03, ...,\n",
       "         3.74322757e-03, 5.79761015e-03, 3.58187617e-03],\n",
       "        [8.61601770e-01, 4.48696837e-02, 2.94799004e-02, ...,\n",
       "         2.37200074e-02, 2.16408707e-02, 2.35624928e-02],\n",
       "        [7.33821094e-01, 2.96128511e-01, 3.10622394e-01, ...,\n",
       "         2.38043845e-01, 2.49641046e-01, 1.68540850e-01],\n",
       "        ...,\n",
       "        [9.96117830e-01, 1.60441521e-11, 2.68988689e-13, ...,\n",
       "         1.26795015e-12, 7.08115207e-13, 5.21615355e-11],\n",
       "        [4.91456598e-01, 2.76308985e-08, 5.25356114e-09, ...,\n",
       "         1.05755280e-08, 1.51807047e-08, 4.24388062e-08],\n",
       "        [9.44838107e-01, 2.27269879e-06, 3.72494071e-07, ...,\n",
       "         9.73373631e-07, 7.06230537e-07, 3.02037688e-06]],\n",
       "\n",
       "       [[5.47040701e-01, 1.06187575e-01, 8.62876102e-02, ...,\n",
       "         9.79437530e-02, 1.15540855e-01, 1.09569319e-01],\n",
       "        [4.70748484e-01, 2.51658708e-01, 2.25957990e-01, ...,\n",
       "         2.00065374e-01, 2.50048369e-01, 1.79967836e-01],\n",
       "        [4.91951197e-01, 2.99439520e-01, 2.99772650e-01, ...,\n",
       "         2.70253599e-01, 2.82471597e-01, 2.24726379e-01],\n",
       "        ...,\n",
       "        [1.67039454e-01, 5.95905147e-09, 1.24375032e-09, ...,\n",
       "         7.95755739e-10, 7.29088478e-09, 7.96389656e-08],\n",
       "        [5.61118543e-01, 3.43824013e-06, 8.66626124e-07, ...,\n",
       "         1.61777007e-06, 1.80717245e-06, 2.02037372e-05],\n",
       "        [2.99627006e-01, 2.53602284e-05, 1.19177694e-05, ...,\n",
       "         8.35554056e-06, 3.47167261e-05, 1.15600880e-04]],\n",
       "\n",
       "       [[5.64998090e-01, 1.91346198e-01, 1.89421818e-01, ...,\n",
       "         1.75079003e-01, 2.19527811e-01, 1.69812426e-01],\n",
       "        [6.76969111e-01, 2.84086972e-01, 2.83014029e-01, ...,\n",
       "         2.24184275e-01, 2.56286681e-01, 2.11415619e-01],\n",
       "        [6.96905851e-01, 1.38880923e-01, 1.31390944e-01, ...,\n",
       "         1.31232768e-01, 1.44295201e-01, 1.10258929e-01],\n",
       "        ...,\n",
       "        [9.74185705e-01, 1.01360797e-06, 7.87420760e-08, ...,\n",
       "         2.11898168e-07, 1.75805340e-07, 1.45596618e-06],\n",
       "        [5.69519281e-01, 3.30804505e-05, 7.96934273e-06, ...,\n",
       "         1.69602044e-05, 2.58312175e-05, 3.27317503e-05],\n",
       "        [8.75007510e-01, 2.90881813e-04, 8.36618128e-05, ...,\n",
       "         1.43860540e-04, 1.57899165e-04, 4.70934407e-04]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrcnn_mask[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
