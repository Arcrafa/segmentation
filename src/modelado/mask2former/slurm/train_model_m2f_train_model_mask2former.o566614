/var/spool/slurm/job566614/slurm_script: line 18: ï¿¼: command not found
wcgpu04.fnal.gov
GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-0f698262-da8d-d3cb-7d09-e4e3c4b988e2)
GPU 1: Tesla V100-PCIE-32GB (UUID: GPU-dca4cd4b-03e3-1021-36d2-916a1e59be96)
/wclustre/nova/users/rafaelma2/venv385/bin/python
/wclustre/nova/users/rafaelma2/venv385/lib/python3.8/site-packages/transformers/models/maskformer/image_processing_maskformer.py:419: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.
  warnings.warn(
Some weights of the model checkpoint at /wclustre/nova/users/rafaelma2/NOvA-Clean/modelos/m2fpre were not used when initializing Mask2FormerForUniversalSegmentation: ['model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.3.fc2.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.transformer_module.decoder.layers.3.final_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.6.fc2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.transformer_module.decoder.layers.6.self_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight', 'model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias', 'model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.5.fc2.bias', 'model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias', 'model.transformer_module.decoder.layers.3.fc1.bias', 'model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight', 'model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.decoder.encoder.layers.3.fc1.weight', 'model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias', 'model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias', 'model.transformer_module.decoder.layers.2.fc2.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.4.fc1.weight', 'model.transformer_module.decoder.layers.4.self_attn.v_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.transformer_module.decoder.layers.3.self_attn.v_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight', 'model.transformer_module.decoder.layers.8.self_attn.k_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.transformer_module.decoder.layers.7.self_attn.v_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.5.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias', 'model.transformer_module.decoder.layers.5.fc2.bias', 'model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.3.self_attn.v_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.2.fc1.weight', 'model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.4.fc1.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight', 'model.transformer_module.decoder.layers.5.fc2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.transformer_module.decoder.layers.6.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.8.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.transformer_module.decoder.layers.4.self_attn.v_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.8.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.pixel_level_module.decoder.encoder.layers.3.fc2.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias', 'model.transformer_module.decoder.layers.4.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight', 'model.transformer_module.decoder.layers.7.final_layer_norm.weight', 'model.transformer_module.decoder.layers.5.self_attn.k_proj.bias', 'model.transformer_module.decoder.layers.3.self_attn.q_proj.weight', 'model.transformer_module.decoder.layers.2.self_attn.v_proj.bias', 'model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias', 'model.transformer_module.decoder.layers.8.self_attn.q_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.transformer_module.decoder.layers.7.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.2.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.transformer_module.decoder.layers.5.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias', 'model.transformer_module.decoder.layers.3.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.8.self_attn.q_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.4.fc1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight', 'model.transformer_module.decoder.layers.8.final_layer_norm.bias', 'model.transformer_module.decoder.layers.4.fc2.bias', 'model.transformer_module.decoder.layers.3.self_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight', 'model.transformer_module.decoder.layers.5.self_attn.out_proj.bias', 'model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.6.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.transformer_module.decoder.layers.5.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.7.final_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.transformer_module.decoder.layers.6.self_attn.q_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias', 'model.transformer_module.decoder.layers.2.self_attn.k_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.decoder.encoder.layers.5.fc2.weight', 'model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight', 'model.transformer_module.decoder.layers.3.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.5.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.5.fc1.weight', 'model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight', 'model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias', 'model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.transformer_module.decoder.layers.4.fc2.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.8.self_attn.out_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias', 'model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight', 'model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight', 'model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.transformer_module.decoder.layers.5.self_attn.q_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.2.self_attn.k_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.transformer_module.decoder.layers.4.self_attn.k_proj.bias', 'model.transformer_module.decoder.layers.7.fc2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.transformer_module.decoder.layers.6.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.pixel_level_module.decoder.encoder.layers.2.fc2.weight', 'model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.pixel_level_module.decoder.encoder.layers.2.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.2.fc2.bias', 'model.transformer_module.decoder.layers.3.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.transformer_module.decoder.layers.3.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight', 'model.transformer_module.decoder.layers.2.final_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight', 'model.transformer_module.decoder.layers.4.self_attn.out_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias', 'model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight', 'model.transformer_module.decoder.layers.5.final_layer_norm.weight', 'model.transformer_module.decoder.layers.3.fc1.weight', 'model.pixel_level_module.decoder.encoder.layers.3.fc1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias', 'model.transformer_module.decoder.layers.8.final_layer_norm.weight', 'model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias', 'model.transformer_module.decoder.layers.8.fc1.weight', 'model.transformer_module.decoder.layers.5.fc1.bias', 'model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias', 'model.transformer_module.decoder.layers.7.self_attn.out_proj.bias', 'model.transformer_module.decoder.layers.4.self_attn.q_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.transformer_module.decoder.layers.8.fc2.weight', 'model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.7.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight', 'model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight', 'model.transformer_module.decoder.layers.6.final_layer_norm.weight', 'model.transformer_module.decoder.layers.2.self_attn.out_proj.bias', 'model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight', 'model.transformer_module.decoder.layers.3.fc2.weight', 'model.transformer_module.decoder.layers.6.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias', 'model.transformer_module.decoder.layers.6.self_attn.q_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.transformer_module.decoder.layers.4.self_attn.k_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.5.fc1.bias', 'model.transformer_module.decoder.layers.7.self_attn.v_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.transformer_module.decoder.layers.7.self_attn.q_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.8.fc1.bias', 'model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight', 'model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.2.self_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.transformer_module.decoder.layers.2.self_attn.q_proj.weight', 'model.transformer_module.decoder.layers.2.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.6.fc1.bias', 'model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias', 'model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight', 'model.transformer_module.decoder.layers.7.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.transformer_module.decoder.layers.7.self_attn.k_proj.bias', 'model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.5.fc1.weight', 'model.transformer_module.decoder.layers.6.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.2.fc1.bias', 'model.transformer_module.decoder.layers.7.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.6.self_attn.k_proj.bias', 'model.transformer_module.decoder.layers.6.self_attn.v_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.transformer_module.decoder.layers.6.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.8.self_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.transformer_module.decoder.layers.8.self_attn.v_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.3.self_attn.k_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.transformer_module.decoder.layers.2.fc2.weight', 'model.transformer_module.decoder.layers.4.final_layer_norm.weight', 'model.transformer_module.decoder.layers.7.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.4.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.4.fc2.weight', 'model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight', 'model.transformer_module.decoder.layers.7.fc1.bias', 'model.transformer_module.decoder.layers.2.fc1.bias', 'model.transformer_module.decoder.layers.5.self_attn.v_proj.bias', 'model.transformer_module.decoder.layers.8.self_attn.v_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.4.fc2.bias', 'model.transformer_module.decoder.layers.4.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.4.final_layer_norm.bias', 'model.transformer_module.decoder.layers.5.self_attn.q_proj.weight', 'model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.transformer_module.decoder.layers.3.final_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight', 'model.transformer_module.decoder.layers.2.self_attn.q_proj.bias']
- This IS expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at /wclustre/nova/users/rafaelma2/NOvA-Clean/modelos/m2fpre and are newly initialized because the shapes did not match:
- model.transformer_module.queries_embedder.weight: found shape torch.Size([100, 256]) in the checkpoint and torch.Size([30, 256]) in the model instantiated
- model.transformer_module.queries_features.weight: found shape torch.Size([100, 256]) in the checkpoint and torch.Size([30, 256]) in the model instantiated
- class_predictor.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated
- class_predictor.bias: found shape torch.Size([81]) in the checkpoint and torch.Size([6]) in the model instantiated
- criterion.empty_weight: found shape torch.Size([81]) in the checkpoint and torch.Size([6]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name  | Type                                | Params
--------------------------------------------------------------
0 | model | Mask2FormerForUniversalSegmentation | 16.9 M
--------------------------------------------------------------
16.9 M    Trainable params
0         Non-trainable params
16.9 M    Total params
67.416    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Loaded 816 images.
Loaded 22 images.
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking:   0%|                                                                                           | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|                                                                              | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.03s/it]                                                                                                                                       /wclustre/nova/users/rafaelma2/venv385/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 11. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/wclustre/nova/users/rafaelma2/venv385/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/wclustre/nova/users/rafaelma2/venv385/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training:   0%|                                                                                                  | 0/7 [00:00<?, ?it/s]Epoch 0:   0%|                                                                                                   | 0/7 [00:00<?, ?it/s]Epoch 0:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                              | 1/7 [00:02<00:16,  2.72s/it]Epoch 0:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                      | 1/7 [00:02<00:16,  2.72s/it, v_num=1]Epoch 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 2/7 [00:03<00:07,  1.58s/it, v_num=1]Epoch 0:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                          | 2/7 [00:03<00:07,  1.58s/it, v_num=1]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 3/7 [00:03<00:04,  1.24s/it, v_num=1]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                              | 3/7 [00:03<00:04,  1.24s/it, v_num=1]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 4/7 [00:04<00:03,  1.07s/it, v_num=1]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 4/7 [00:04<00:03,  1.07s/it, v_num=1]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 5/7 [00:04<00:01,  1.03it/s, v_num=1]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 5/7 [00:04<00:01,  1.03it/s, v_num=1]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 6/7 [00:05<00:00,  1.11it/s, v_num=1]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 6/7 [00:05<00:00,  1.11it/s, v_num=1]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:05<00:00,  1.18it/s, v_num=1]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:05<00:00,  1.18it/s, v_num=1]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|                                                                                                | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|                                                                                   | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.95it/s][AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.02it/s, v_num=1]
                                                                                                                                       [AEpoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:06<00:00,  1.02it/s, v_num=1]`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:07<00:00,  1.03s/it, v_num=1]
