/var/spool/slurm/job566612/slurm_script: line 18: ï¿¼: command not found
wcgpu04.fnal.gov
GPU 0: Tesla V100-PCIE-32GB (UUID: GPU-0f698262-da8d-d3cb-7d09-e4e3c4b988e2)
GPU 1: Tesla V100-PCIE-32GB (UUID: GPU-dca4cd4b-03e3-1021-36d2-916a1e59be96)
/wclustre/nova/users/rafaelma2/venv385/bin/python
/wclustre/nova/users/rafaelma2/venv385/lib/python3.8/site-packages/transformers/models/maskformer/image_processing_maskformer.py:419: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.
  warnings.warn(
Some weights of the model checkpoint at /wclustre/nova/users/rafaelma2/NOvA-Clean/modelos/m2fpre were not used when initializing Mask2FormerForUniversalSegmentation: ['model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias', 'model.transformer_module.decoder.layers.7.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias', 'model.transformer_module.decoder.layers.7.final_layer_norm.weight', 'model.transformer_module.decoder.layers.6.self_attn.k_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias', 'model.transformer_module.decoder.layers.5.self_attn.k_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight', 'model.transformer_module.decoder.layers.7.cross_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.8.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.6.fc2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias', 'model.transformer_module.decoder.layers.3.self_attn.q_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias', 'model.transformer_module.decoder.layers.6.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias', 'model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight', 'model.transformer_module.decoder.layers.4.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.6.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight', 'model.transformer_module.decoder.layers.2.self_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias', 'model.transformer_module.decoder.layers.7.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.3.cross_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight', 'model.transformer_module.decoder.layers.5.fc2.bias', 'model.transformer_module.decoder.layers.5.final_layer_norm.weight', 'model.transformer_module.decoder.layers.5.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.4.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias', 'model.transformer_module.decoder.layers.2.cross_attn.out_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias', 'model.transformer_module.decoder.layers.7.final_layer_norm.bias', 'model.transformer_module.decoder.layers.4.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias', 'model.transformer_module.decoder.layers.7.self_attn.v_proj.bias', 'model.transformer_module.decoder.layers.6.cross_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.6.self_attn.q_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias', 'model.pixel_level_module.decoder.encoder.layers.4.fc1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight', 'model.transformer_module.decoder.layers.3.fc1.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.4.fc1.bias', 'model.transformer_module.decoder.layers.4.self_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.3.fc2.weight', 'model.transformer_module.decoder.layers.5.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.3.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.2.final_layer_norm.bias', 'model.transformer_module.decoder.layers.6.self_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight', 'model.transformer_module.decoder.layers.8.self_attn.v_proj.bias', 'model.transformer_module.decoder.layers.3.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.3.final_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.3.fc1.bias', 'model.transformer_module.decoder.layers.4.cross_attn.in_proj_weight', 'model.transformer_module.decoder.layers.3.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight', 'model.pixel_level_module.decoder.encoder.layers.2.fc2.weight', 'model.transformer_module.decoder.layers.3.fc2.weight', 'model.transformer_module.decoder.layers.8.cross_attn.in_proj_bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_index', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight', 'model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight', 'model.transformer_module.decoder.layers.4.self_attn.k_proj.weight', 'model.transformer_module.decoder.layers.8.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight', 'model.transformer_module.decoder.layers.8.cross_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight', 'model.transformer_module.decoder.layers.8.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.3.cross_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight', 'model.transformer_module.decoder.layers.6.final_layer_norm.weight', 'model.transformer_module.decoder.layers.5.self_attn.v_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias', 'model.transformer_module.decoder.layers.7.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias', 'model.transformer_module.decoder.layers.8.self_attn.q_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight', 'model.transformer_module.decoder.layers.4.self_attn.q_proj.weight', 'model.transformer_module.decoder.layers.8.self_attn.k_proj.weight', 'model.transformer_module.decoder.layers.4.cross_attn.in_proj_bias', 'model.transformer_module.decoder.layers.3.self_attn.out_proj.bias', 'model.transformer_module.decoder.layers.7.fc1.bias', 'model.transformer_module.decoder.layers.7.fc1.weight', 'model.transformer_module.decoder.layers.5.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight', 'model.transformer_module.decoder.layers.2.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.3.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight', 'model.transformer_module.decoder.layers.7.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight', 'model.transformer_module.decoder.layers.2.cross_attn.in_proj_bias', 'model.transformer_module.decoder.layers.6.self_attn.v_proj.bias', 'model.transformer_module.decoder.layers.5.self_attn.k_proj.weight', 'model.transformer_module.decoder.layers.2.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight', 'model.transformer_module.decoder.layers.5.fc2.weight', 'model.transformer_module.decoder.layers.2.cross_attn.in_proj_weight', 'model.transformer_module.decoder.layers.7.fc2.weight', 'model.transformer_module.decoder.layers.8.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.2.self_attn.k_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias', 'model.pixel_level_module.decoder.encoder.layers.2.fc1.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias', 'model.transformer_module.decoder.layers.4.self_attn.v_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias', 'model.transformer_module.decoder.layers.2.cross_attn.out_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight', 'model.transformer_module.decoder.layers.2.cross_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.6.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight', 'model.transformer_module.decoder.layers.3.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight', 'model.transformer_module.decoder.layers.8.cross_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.3.self_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias', 'model.transformer_module.decoder.layers.6.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias', 'model.pixel_level_module.decoder.encoder.layers.5.fc2.weight', 'model.transformer_module.decoder.layers.6.self_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.5.self_attn_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias', 'model.transformer_module.decoder.layers.7.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias', 'model.transformer_module.decoder.layers.2.fc1.bias', 'model.transformer_module.decoder.layers.8.cross_attn.in_proj_weight', 'model.transformer_module.decoder.layers.8.final_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias', 'model.transformer_module.decoder.layers.6.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias', 'model.transformer_module.decoder.layers.3.self_attn.q_proj.weight', 'model.transformer_module.decoder.layers.2.final_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight', 'model.transformer_module.decoder.layers.4.cross_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.6.cross_attn.in_proj_bias', 'model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_index', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight', 'model.transformer_module.decoder.layers.6.self_attn.out_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias', 'model.transformer_module.decoder.layers.3.self_attn.v_proj.bias', 'model.transformer_module.decoder.layers.3.cross_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.3.fc2.bias', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias', 'model.transformer_module.decoder.layers.5.cross_attn.in_proj_weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight', 'model.transformer_module.decoder.layers.4.cross_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.5.fc1.bias', 'model.transformer_module.decoder.layers.3.cross_attn.in_proj_weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias', 'model.transformer_module.decoder.layers.4.self_attn.out_proj.bias', 'model.transformer_module.decoder.layers.4.fc2.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias', 'model.transformer_module.decoder.layers.5.fc1.weight', 'model.transformer_module.decoder.layers.5.cross_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.8.cross_attn.out_proj.weight', 'model.transformer_module.decoder.layers.5.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.7.cross_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias', 'model.transformer_module.decoder.layers.8.fc1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias', 'model.transformer_module.decoder.layers.8.self_attn.v_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight', 'model.transformer_module.decoder.layers.4.self_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight', 'model.transformer_module.decoder.layers.7.cross_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.6.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias', 'model.transformer_module.decoder.layers.7.self_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias', 'model.transformer_module.decoder.layers.6.cross_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight', 'model.transformer_module.decoder.layers.8.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.5.cross_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.3.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight', 'model.transformer_module.decoder.layers.3.final_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight', 'model.transformer_module.decoder.layers.8.self_attn.out_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias', 'model.transformer_module.decoder.layers.2.self_attn.v_proj.bias', 'model.transformer_module.decoder.layers.4.cross_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight', 'model.transformer_module.decoder.layers.7.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.4.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight', 'model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias', 'model.transformer_module.decoder.layers.8.self_attn.k_proj.bias', 'model.transformer_module.decoder.layers.6.self_attn.q_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias', 'model.transformer_module.decoder.layers.2.self_attn.v_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight', 'model.transformer_module.decoder.layers.2.self_attn.out_proj.weight', 'model.transformer_module.decoder.layers.6.self_attn.k_proj.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.2.self_attn.k_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias', 'model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias', 'model.pixel_level_module.decoder.encoder.layers.5.fc1.bias', 'model.transformer_module.decoder.layers.5.self_attn.out_proj.bias', 'model.transformer_module.decoder.layers.7.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.7.self_attn.k_proj.bias', 'model.pixel_level_module.decoder.encoder.layers.5.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight', 'model.transformer_module.decoder.layers.2.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.5.cross_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight', 'model.transformer_module.decoder.layers.4.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight', 'model.transformer_module.decoder.layers.2.fc2.weight', 'model.pixel_level_module.decoder.encoder.layers.4.fc2.bias', 'model.transformer_module.decoder.layers.4.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.7.self_attn.out_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_index', 'model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight', 'model.transformer_module.decoder.layers.5.cross_attn.in_proj_bias', 'model.pixel_level_module.decoder.encoder.layers.4.fc2.weight', 'model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias', 'model.transformer_module.decoder.layers.4.fc2.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.3.cross_attn.in_proj_bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias', 'model.transformer_module.decoder.layers.5.cross_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight', 'model.transformer_module.decoder.layers.6.self_attn.v_proj.weight', 'model.transformer_module.decoder.layers.2.self_attn.q_proj.bias', 'model.transformer_module.decoder.layers.7.cross_attn.in_proj_bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.8.fc1.weight', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias', 'model.transformer_module.decoder.layers.8.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight', 'model.transformer_module.decoder.layers.6.cross_attn.out_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias', 'model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias', 'model.transformer_module.decoder.layers.2.cross_attn_layer_norm.bias', 'model.transformer_module.decoder.layers.5.self_attn_layer_norm.weight', 'model.transformer_module.decoder.layers.2.fc2.bias', 'model.transformer_module.decoder.layers.4.self_attn.k_proj.bias', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight', 'model.pixel_level_module.decoder.encoder.layers.2.fc1.bias', 'model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight', 'model.transformer_module.decoder.layers.5.self_attn.q_proj.weight', 'model.transformer_module.decoder.layers.7.self_attn.q_proj.weight', 'model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight', 'model.transformer_module.decoder.layers.3.self_attn_layer_norm.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_index', 'model.transformer_module.decoder.layers.4.final_layer_norm.weight', 'model.transformer_module.decoder.layers.2.self_attn.q_proj.weight', 'model.transformer_module.decoder.layers.7.self_attn_layer_norm.weight', 'model.pixel_level_module.decoder.encoder.layers.2.fc2.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight', 'model.transformer_module.decoder.layers.6.fc1.bias', 'model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias', 'model.pixel_level_module.decoder.encoder.layers.5.fc1.weight', 'model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table', 'model.transformer_module.decoder.layers.3.self_attn.k_proj.bias', 'model.transformer_module.decoder.layers.8.fc2.weight', 'model.transformer_module.decoder.layers.3.fc2.bias', 'model.pixel_level_module.decoder.encoder.layers.4.fc1.weight', 'model.transformer_module.decoder.layers.8.self_attn.out_proj.bias']
- This IS expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Mask2FormerForUniversalSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at /wclustre/nova/users/rafaelma2/NOvA-Clean/modelos/m2fpre and are newly initialized because the shapes did not match:
- model.transformer_module.queries_embedder.weight: found shape torch.Size([100, 256]) in the checkpoint and torch.Size([30, 256]) in the model instantiated
- model.transformer_module.queries_features.weight: found shape torch.Size([100, 256]) in the checkpoint and torch.Size([30, 256]) in the model instantiated
- class_predictor.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated
- class_predictor.bias: found shape torch.Size([81]) in the checkpoint and torch.Size([6]) in the model instantiated
- criterion.empty_weight: found shape torch.Size([81]) in the checkpoint and torch.Size([6]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

Missing logger folder: /wclustre/nova/users/rafaelma2/NOvA-Clean/modelos/tb_logs/maskformernova_ag
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name  | Type                                | Params
--------------------------------------------------------------
0 | model | Mask2FormerForUniversalSegmentation | 16.9 M
--------------------------------------------------------------
16.9 M    Trainable params
0         Non-trainable params
16.9 M    Total params
67.416    Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Loaded 142494 images.
Loaded 17650 images.
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking:   0%|                                                                                           | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|                                                                              | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|âââââââââââââââââââââââââââââââââââ                                   | 1/2 [00:03<00:03,  3.03s/it]Sanity Checking DataLoader 0: 100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2/2 [00:03<00:00,  1.86s/it]                                                                                                                                       /wclustre/nova/users/rafaelma2/venv385/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 60. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
/wclustre/nova/users/rafaelma2/venv385/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
Training: 0it [00:00, ?it/s]Training:   0%|                                                                                               | 0/1188 [00:00<?, ?it/s]Epoch 0:   0%|                                                                                                | 0/1188 [00:00<?, ?it/s]Epoch 0:   0%|                                                                                      | 1/1188 [00:05<1:42:50,  5.20s/it]Epoch 0:   0%|                                                                             | 1/1188 [00:05<1:42:51,  5.20s/it, v_num=0]Epoch 0:   0%|â                                                                            | 2/1188 [00:06<1:05:49,  3.33s/it, v_num=0]Epoch 0:   0%|â                                                                            | 2/1188 [00:06<1:05:49,  3.33s/it, v_num=0]Epoch 0:   0%|â                                                                              | 3/1188 [00:07<49:52,  2.53s/it, v_num=0]Epoch 0:   0%|â                                                                              | 3/1188 [00:07<49:52,  2.53s/it, v_num=0]Epoch 0:   0%|â                                                                              | 4/1188 [00:08<41:55,  2.12s/it, v_num=0]Epoch 0:   0%|â                                                                              | 4/1188 [00:08<41:55,  2.12s/it, v_num=0]Epoch 0:   0%|â                                                                              | 5/1188 [00:09<37:10,  1.89s/it, v_num=0]Epoch 0:   0%|â                                                                              | 5/1188 [00:09<37:10,  1.89s/it, v_num=0]Epoch 0:   1%|â                                                                              | 6/1188 [00:10<33:45,  1.71s/it, v_num=0]Epoch 0:   1%|â                                                                              | 6/1188 [00:10<33:45,  1.71s/it, v_num=0]Epoch 0:   1%|â                                                                              | 7/1188 [00:11<31:22,  1.59s/it, v_num=0]Epoch 0:   1%|â                                                                              | 7/1188 [00:11<31:22,  1.59s/it, v_num=0]Epoch 0:   1%|â                                                                              | 8/1188 [00:12<29:33,  1.50s/it, v_num=0]Epoch 0:   1%|â                                                                              | 8/1188 [00:12<29:33,  1.50s/it, v_num=0]Epoch 0:   1%|â                                                                              | 9/1188 [00:12<28:13,  1.44s/it, v_num=0]Epoch 0:   1%|â                                                                              | 9/1188 [00:12<28:13,  1.44s/it, v_num=0]Epoch 0:   1%|â                                                                             | 10/1188 [00:13<27:07,  1.38s/it, v_num=0]Epoch 0:   1%|â                                                                             | 10/1188 [00:13<27:07,  1.38s/it, v_num=0]Epoch 0:   1%|â                                                                             | 11/1188 [00:14<26:16,  1.34s/it, v_num=0]Epoch 0:   1%|â                                                                             | 11/1188 [00:14<26:16,  1.34s/it, v_num=0]Epoch 0:   1%|â                                                                             | 12/1188 [00:15<25:36,  1.31s/it, v_num=0]Epoch 0:   1%|â                                                                             | 12/1188 [00:15<25:36,  1.31s/it, v_num=0]Epoch 0:   1%|â                                                                             | 13/1188 [00:16<25:00,  1.28s/it, v_num=0]Epoch 0:   1%|â                                                                             | 13/1188 [00:16<25:00,  1.28s/it, v_num=0]Epoch 0:   1%|â                                                                             | 14/1188 [00:17<24:07,  1.23s/it, v_num=0]Epoch 0:   1%|â                                                                             | 14/1188 [00:17<24:07,  1.23s/it, v_num=0]Epoch 0:   1%|â                                                                             | 15/1188 [00:17<23:17,  1.19s/it, v_num=0]Epoch 0:   1%|â                                                                             | 15/1188 [00:17<23:17,  1.19s/it, v_num=0]Epoch 0:   1%|â                                                                             | 16/1188 [00:18<22:32,  1.15s/it, v_num=0]Epoch 0:   1%|â                                                                             | 16/1188 [00:18<22:32,  1.15s/it, v_num=0]Epoch 0:   1%|â                                                                             | 17/1188 [00:19<21:53,  1.12s/it, v_num=0]Epoch 0:   1%|â                                                                             | 17/1188 [00:19<21:53,  1.12s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 18/1188 [00:19<21:17,  1.09s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 18/1188 [00:19<21:17,  1.09s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 19/1188 [00:20<20:46,  1.07s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 19/1188 [00:20<20:46,  1.07s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 20/1188 [00:20<20:18,  1.04s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 20/1188 [00:20<20:18,  1.04s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 21/1188 [00:21<19:52,  1.02s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 21/1188 [00:21<19:52,  1.02s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 22/1188 [00:22<19:29,  1.00s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 22/1188 [00:22<19:29,  1.00s/it, v_num=0]Epoch 0:   2%|ââ                                                                            | 23/1188 [00:22<19:07,  1.02it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 23/1188 [00:22<19:07,  1.02it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 24/1188 [00:23<18:48,  1.03it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 24/1188 [00:23<18:48,  1.03it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 25/1188 [00:23<18:28,  1.05it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 25/1188 [00:23<18:28,  1.05it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 26/1188 [00:24<18:11,  1.06it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 26/1188 [00:24<18:11,  1.06it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 27/1188 [00:25<17:56,  1.08it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 27/1188 [00:25<17:56,  1.08it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 28/1188 [00:25<17:41,  1.09it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 28/1188 [00:25<17:41,  1.09it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 29/1188 [00:26<17:27,  1.11it/s, v_num=0]Epoch 0:   2%|ââ                                                                            | 29/1188 [00:26<17:27,  1.11it/s, v_num=0]Epoch 0:   3%|ââ                                                                            | 30/1188 [00:26<17:15,  1.12it/s, v_num=0]Epoch 0:   3%|ââ                                                                            | 30/1188 [00:26<17:15,  1.12it/s, v_num=0]Epoch 0:   3%|ââ                                                                            | 31/1188 [00:27<17:04,  1.13it/s, v_num=0]Epoch 0:   3%|ââ                                                                            | 31/1188 [00:27<17:04,  1.13it/s, v_num=0]Epoch 0:   3%|ââ                                                                            | 32/1188 [00:28<16:53,  1.14it/s, v_num=0]Epoch 0:   3%|ââ                                                                            | 32/1188 [00:28<16:54,  1.14it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 33/1188 [00:28<16:44,  1.15it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 33/1188 [00:28<16:44,  1.15it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 34/1188 [00:29<16:34,  1.16it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 34/1188 [00:29<16:34,  1.16it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 35/1188 [00:29<16:25,  1.17it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 35/1188 [00:29<16:25,  1.17it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 36/1188 [00:30<16:15,  1.18it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 36/1188 [00:30<16:15,  1.18it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 37/1188 [00:31<16:07,  1.19it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 37/1188 [00:31<16:07,  1.19it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 38/1188 [00:31<15:59,  1.20it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 38/1188 [00:31<15:59,  1.20it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 39/1188 [00:32<15:51,  1.21it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 39/1188 [00:32<15:51,  1.21it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 40/1188 [00:32<15:43,  1.22it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 40/1188 [00:32<15:43,  1.22it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 41/1188 [00:33<15:36,  1.22it/s, v_num=0]Epoch 0:   3%|âââ                                                                           | 41/1188 [00:33<15:36,  1.22it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 42/1188 [00:34<15:30,  1.23it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 42/1188 [00:34<15:30,  1.23it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 43/1188 [00:34<15:23,  1.24it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 43/1188 [00:34<15:23,  1.24it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 44/1188 [00:35<15:17,  1.25it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 44/1188 [00:35<15:17,  1.25it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 45/1188 [00:35<15:11,  1.25it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 45/1188 [00:35<15:11,  1.25it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 46/1188 [00:36<15:05,  1.26it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 46/1188 [00:36<15:05,  1.26it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 47/1188 [00:37<15:00,  1.27it/s, v_num=0]Epoch 0:   4%|âââ                                                                           | 47/1188 [00:37<15:00,  1.27it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 48/1188 [00:37<14:55,  1.27it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 48/1188 [00:37<14:55,  1.27it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 49/1188 [00:38<14:49,  1.28it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 49/1188 [00:38<14:49,  1.28it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 50/1188 [00:38<14:44,  1.29it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 50/1188 [00:38<14:44,  1.29it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 51/1188 [00:39<14:39,  1.29it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 51/1188 [00:39<14:39,  1.29it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 52/1188 [00:40<14:35,  1.30it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 52/1188 [00:40<14:35,  1.30it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 53/1188 [00:40<14:30,  1.30it/s, v_num=0]Epoch 0:   4%|ââââ                                                                          | 53/1188 [00:40<14:30,  1.30it/s, v_num=0]Epoch 0:   5%|ââââ                                                                          | 54/1188 [00:41<14:26,  1.31it/s, v_num=0]Epoch 0:   5%|ââââ                                                                          | 54/1188 [00:41<14:26,  1.31it/s, v_num=0]slurmstepd: error: *** JOB 566612 ON wcgpu04 CANCELLED AT 2024-06-08T19:20:16 ***
